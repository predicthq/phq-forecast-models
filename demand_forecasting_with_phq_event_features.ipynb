{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to apply PredictHQ’s model in real forecasting scenarios, guiding users through feature engineering, model evaluation, and forecasting. It compares a baseline model (using only time trends) with a PredictHQ-enhanced model to quantify the value of event intelligence.\n",
    "\n",
    "A sample demand dataset and configuration file is provided.\n",
    "\n",
    "Please refer to the `README.md` file for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "* [Settings](#Settings)\n",
    "* [Load Demand and Configuration Files](#Load-Demand-and-Configuration-files)\n",
    "* [Beam Analysis and Feature Engineering](#Beam-Analysis-and-Feature-Engineering)\n",
    "* [Model Evaluation and Comparison](#Model-Evaluation-and-Comparison)\n",
    "* [Model Creation](#Model-Creation)\n",
    "* [Forecasting](#Forecasting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies if not already installed\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import cloudpickle\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from phq import (\n",
    "    run_beam_analysis,\n",
    "    process_demand_data,\n",
    "    prepare_event_features,\n",
    "    prepare_time_trend_features,\n",
    "    prepare_forecast_features,\n",
    "    evaluate_forecast_model,\n",
    "    PhqForecastModel\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings\n",
    "\n",
    "The notebook supports two execution modes, controlled by the `RUN_SETTING` parameter:\n",
    "\n",
    "1. `RUN_SETTING = \"CSV_EVENT_FEATURES\"` – Runs without a `PHQ_ACCESS_TOKEN`, using pre-generated event feature files for the provided sample demand dataset and configuration file.\n",
    "2. `RUN_SETTING = \"API_EVENT_FEATURES\"` – Runs with a `PHQ_ACCESS_TOKEN`, using PredictHQ APIs to generate PredictHQ Event Features for the provided demand dataset and configuration file. The demand dataset can be either the sample demand data or the user’s own dataset.\n",
    "\n",
    "Ensure that `PHQ_ACCESS_TOKEN` is set when selecting `API_EVENT_FEATURES`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the run setting to \"CSV_EVENT_FEATURES\" to use the sample csv files in the data folder or \"API_EVENT_FEATURES\" to use the PredictHQ APIs\n",
    "RUN_SETTING = \"CSV_EVENT_FEATURES\"\n",
    "if RUN_SETTING == \"API_EVENT_FEATURES\":\n",
    "    # set the PHQ access token in the environment variable or replace \"XXXXXX\" with your access token\n",
    "    PHQ_ACCESS_TOKEN = os.environ.get(\"PHQ_ACCESS_TOKEN\") or \"XXXXXX\"\n",
    "\n",
    "if RUN_SETTING == \"CSV_EVENT_FEATURES\":\n",
    "    print(\"Running the notebook with provided sample demand and PredictHQ Event Features\")\n",
    "else:\n",
    "    print(f\"Running with PHQ_ACCESS_TOKEN\")\n",
    "\n",
    "os.makedirs(\"results/models\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Demand and Configuration Files\n",
    "\n",
    "The demand dataset is stored as a CSV file with columns for `date` and `demand`. The configuration file provides metadata such as `lat`, `lon`, `industry`, and `name`, which are required in the feature engineering step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the sample daily demand data\n",
    "sample_demand_df = pd.read_csv(\"data/sample_demand.csv\")\n",
    "\n",
    "# Read the configuration file\n",
    "with open(\"data/sample_config.json\", \"r\") as json_file:\n",
    "    config = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Beam Analysis and Feature Engineering\n",
    "\n",
    "The PredictHQ Event Features are prepared based on Feature Importance results from a Beam Analysis. The time trend features are based on the time dates and historical demand values. Key steps include:\n",
    "- Running a **Beam Analysis** using PredictHQ API if `RUN_SETTING = \"API_EVENT_FEATURES\"`.\n",
    "- Preparing demand data to handle missing values.\n",
    "- Preparing **PredictHQ Event Features**.\n",
    "- Preparing **time trend features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if RUN_SETTING == \"API_EVENT_FEATURES\":\n",
    "    # create an analysis and wait for it to complete\n",
    "    sample_beam_analysis_result = run_beam_analysis(\n",
    "        config[\"name\"],\n",
    "        config[\"lat\"],\n",
    "        config[\"lon\"],\n",
    "        sample_demand_df,\n",
    "        PHQ_ACCESS_TOKEN,\n",
    "        industry=config[\"industry\"],\n",
    "    )\n",
    "    sample_beam_analysis_id = sample_beam_analysis_result.analysis_id\n",
    "else:\n",
    "    sample_beam_analysis_id = \"csv_sample_beam_analysis_id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process demand dataset\n",
    "demand_df = process_demand_data(sample_demand_df)\n",
    "# Prepare PredictHQ Event Features\n",
    "if RUN_SETTING == \"API_EVENT_FEATURES\":\n",
    "    event_features_df = prepare_event_features(\n",
    "        sample_beam_analysis_id, PHQ_ACCESS_TOKEN\n",
    "    )\n",
    "else:\n",
    "    event_features_df = pd.read_csv(\"data/sample_event_features.csv\")\n",
    "    event_features_df[\"date\"] = pd.to_datetime(event_features_df[\"date\"])\n",
    "# Prepare time trend features\n",
    "time_trend_features_df = prepare_time_trend_features(demand_df)\n",
    "# Combine PredictHQ Event Features and time trend features\n",
    "combined_features_df = time_trend_features_df.merge(event_features_df, on=\"date\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Evaluation and Comparison\n",
    "\n",
    "The model performance is evaluated and compared with and without PredictHQ Event Features using **Mean Absolute Percentage Error (MAPE)**. The model evalution might take a few minutes depending on the size of the demand dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast model evaluation with PredictHQ Event Features\n",
    "evaluation_results_phq = evaluate_forecast_model(combined_features_df, demand_df)\n",
    "\n",
    "# Forecast model evaluation without PredictHQ Event Features\n",
    "evaluation_results_baseline = evaluate_forecast_model(time_trend_features_df, demand_df)\n",
    "\n",
    "# Model performance comparison\n",
    "phq_mape = evaluation_results_phq[\"mape\"]\n",
    "baseline_mape = evaluation_results_baseline[\"mape\"]\n",
    "print(f\"MAPE for forecast model with PredictHQ Event Features: {phq_mape:.2f}\")\n",
    "print(f\"MAPE for forecast model without PredictHQ Event Features: {baseline_mape:.2f}\")\n",
    "print(f\"Relative MAPE improvements: {100 * (baseline_mape - phq_mape) / baseline_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation\n",
    "\n",
    "The model is trained using event and time trend features and then saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forecast model\n",
    "forecast_model = PhqForecastModel()\n",
    "forecast_model.fit(combined_features_df, demand_df)\n",
    "# Save forecast model\n",
    "with open(f\"results/models/model_{sample_beam_analysis_id}.pkl\", \"wb\") as f:\n",
    "    cloudpickle.dump(forecast_model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forecasting\n",
    "\n",
    "Prepare forecasting features and apply the trained model to predict future demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for forecasting\n",
    "FORECAST_HORIZON = 7\n",
    "\n",
    "if RUN_SETTING == \"API_EVENT_FEATURES\":\n",
    "    forecasting_features_df = prepare_forecast_features(\n",
    "        demand_df, sample_beam_analysis_id, FORECAST_HORIZON, PHQ_ACCESS_TOKEN\n",
    "    )\n",
    "else:\n",
    "    forecasting_features_df = pd.read_csv(\"data/sample_forecasting_features.csv\")\n",
    "    forecasting_features_df[\"date\"] = pd.to_datetime(forecasting_features_df[\"date\"])\n",
    "# Forecast demand values\n",
    "predictions = forecast_model.predict(forecasting_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Forecasting Results\n",
    "\n",
    "Visualize and compare historical demand with forecasted demand values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=sample_demand_df[\"date\"],\n",
    "        y=sample_demand_df[\"demand\"],\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Actual\",\n",
    "    )\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=forecasting_features_df[\"date\"],\n",
    "        y=predictions,\n",
    "        mode=\"lines+markers\",\n",
    "        name=\"Forecast\",\n",
    "    )\n",
    ")\n",
    "# Customize layout\n",
    "fig.update_layout(\n",
    "    title=f'Forecast results for the next {FORECAST_HORIZON} days',\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Demand\",\n",
    ")\n",
    "\n",
    "fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecast_model_test_com",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
